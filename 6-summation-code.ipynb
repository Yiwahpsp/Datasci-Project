{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_array_or_dict_values(data, path, key):\n",
    "    \"\"\"Extract values from deeply nested arrays/dictionaries.\"\"\"\n",
    "    try:\n",
    "        value = data\n",
    "        for p in path:\n",
    "            if isinstance(value, dict):\n",
    "                value = value.get(p, None)\n",
    "            elif isinstance(value, list):\n",
    "                # Handle list of dicts or list of lists\n",
    "                temp_values = []\n",
    "                for item in value:\n",
    "                    if isinstance(item, dict):\n",
    "                        item_value = item.get(p, None)\n",
    "                        if isinstance(item_value, list):\n",
    "                            temp_values.extend(item_value)\n",
    "                        elif item_value is not None:\n",
    "                            temp_values.append(item_value)\n",
    "                    elif isinstance(item, list):\n",
    "                        temp_values.extend(item)\n",
    "                value = temp_values if temp_values else None\n",
    "            else:\n",
    "                return [None]\n",
    "\n",
    "        # Handle final value\n",
    "        if value is None:\n",
    "            return [None]\n",
    "        elif isinstance(value, dict):\n",
    "            # Handle dictionary case\n",
    "            if key in value:\n",
    "                return [value[key]]\n",
    "            elif '$' in value:  # Special case for $ key\n",
    "                return [value['$']]\n",
    "            return [None]\n",
    "        elif isinstance(value, list):\n",
    "            # Handle list case\n",
    "            results = []\n",
    "            for item in value:\n",
    "                if isinstance(item, dict):\n",
    "                    if key in item:\n",
    "                        results.append(item[key])\n",
    "                    elif '$' in item:  # Special case for $ key\n",
    "                        results.append(item['$'])\n",
    "                    else:\n",
    "                        results.append(None)\n",
    "                else:\n",
    "                    results.append(item if item is not None else None)\n",
    "            return results\n",
    "        else:\n",
    "            return [value]\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting from path {path}: {e}\")\n",
    "        return [None]\n",
    "\n",
    "def extract_features(json_file):\n",
    "    with open(json_file, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    abs_resp = data.get('abstracts-retrieval-response', {})\n",
    "    \n",
    "    # Special handling for organizations\n",
    "    def get_organizations(data):\n",
    "        orgs = []\n",
    "        author_groups = data.get('item', {}).get('bibrecord', {}).get('head', {}).get('author-group', [])\n",
    "        \n",
    "        if isinstance(author_groups, dict):\n",
    "            author_groups = [author_groups]\n",
    "            \n",
    "        for group in author_groups:\n",
    "            if isinstance(group, dict):\n",
    "                affiliation = group.get('affiliation', {})\n",
    "                if isinstance(affiliation, dict):\n",
    "                    org = affiliation.get('organization', [])\n",
    "                    if isinstance(org, dict):\n",
    "                        org = [org]\n",
    "                    if isinstance(org, list):\n",
    "                        for o in org:\n",
    "                            if isinstance(o, dict):\n",
    "                                orgs.append(o.get('$', None))\n",
    "                            else:\n",
    "                                orgs.append(o)\n",
    "        \n",
    "        return orgs if orgs else [None]\n",
    "\n",
    "    features = {\n",
    "        'organizations': {\n",
    "            'custom': get_organizations\n",
    "        },\n",
    "        'classifications': {\n",
    "            'path': ['item', 'bibrecord', 'head', 'enhancement', \n",
    "                    'classificationgroup', 'classifications'],\n",
    "            'key': '@type'\n",
    "        },\n",
    "        'affiliations': {\n",
    "            'path': ['affiliation'],\n",
    "            'key': 'affilname'\n",
    "        },\n",
    "        'auth-keywords': {\n",
    "            'path': ['authkeywords', 'author-keyword'],\n",
    "            'key': '$'\n",
    "        },\n",
    "        'subjects': {\n",
    "            'path': ['subject-areas', 'subject-area'],\n",
    "            'key': '$'\n",
    "        },\n",
    "        'authors': {\n",
    "            'path': ['authors', 'author'],\n",
    "            'key': 'ce:indexed-name'\n",
    "        }\n",
    "    }\n",
    "\n",
    "    record = {'file': Path(json_file).name}\n",
    "\n",
    "    for feature_name, config in features.items():\n",
    "        if 'custom' in config:\n",
    "            # Use custom extraction function for organizations\n",
    "            record[feature_name] = config['custom'](abs_resp)\n",
    "        elif 'subpath' in config:\n",
    "            values = []\n",
    "            main_array = get_array_or_dict_values(abs_resp, config['path'], None)\n",
    "            for item in main_array:\n",
    "                if isinstance(item, dict):\n",
    "                    sub_values = get_array_or_dict_values(item, config['subpath'], config['key'])\n",
    "                    values.extend(sub_values)\n",
    "            record[feature_name] = values\n",
    "        else:\n",
    "            record[feature_name] = get_array_or_dict_values(abs_resp, config['path'], config['key'])\n",
    "\n",
    "    return record\n",
    "  \n",
    "\n",
    "# Process all JSON files and merge features into a DataFrame\n",
    "def process_json_files(root_folder_path):\n",
    "    \"\"\"Process all JSON files and extract features.\"\"\"\n",
    "    all_records = []\n",
    "    root_path = Path(root_folder_path)\n",
    "\n",
    "    # Define expected columns\n",
    "    expected_columns = [\n",
    "        'file', 'organizations', 'classifications', \n",
    "        'affiliations', 'auth-keywords',\n",
    "        'subjects', 'authors'\n",
    "    ]\n",
    "\n",
    "    for json_file in root_path.rglob('*.json'):\n",
    "        try:\n",
    "            record = extract_features(json_file)\n",
    "            # Ensure all expected columns exist with NaN as default\n",
    "            for col in expected_columns:\n",
    "                if col not in record:\n",
    "                    record[col] = np.nan\n",
    "                elif isinstance(record[col], list) and not record[col]:\n",
    "                    record[col] = np.nan\n",
    "            all_records.append(record)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {json_file}: {e}\")\n",
    "            # Add empty record with NaN values if file processing fails\n",
    "            empty_record = {\n",
    "                'file': Path(json_file).name,\n",
    "                **{col: np.nan for col in expected_columns if col != 'file'}\n",
    "            }\n",
    "            all_records.append(empty_record)\n",
    "\n",
    "    if all_records:\n",
    "        df = pd.DataFrame(all_records)\n",
    "        # Combine features into single rows by grouping\n",
    "        grouped_df = (\n",
    "            df.groupby('file').agg(\n",
    "                {col: lambda x: ', '.join(filter(None, x.dropna().astype(str))) for col in df.columns if col != 'file'}\n",
    "            ).reset_index()\n",
    "        )\n",
    "        output_file = 'data/features_separate.csv'\n",
    "        grouped_df.to_csv(output_file, index=False, encoding='utf-8')\n",
    "        print(f\"Processed {len(all_records)} files to {output_file}\")\n",
    "        return grouped_df\n",
    "    else:\n",
    "        print(\"No records found\")\n",
    "        return pd.DataFrame(columns=expected_columns)\n",
    "\n",
    "root_folder = 'raw-data'  # Replace with your JSON root folder\n",
    "df = process_json_files(root_folder)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data_file = pd.read_csv(\"data/features_summation.csv\", encoding='utf-8')\n",
    "df_data_file.info()\n",
    "df_data_file.describe(include='all')\n",
    "df_data_file.shape\n",
    "df_data_file.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data_file.drop(columns=['file', 'authors', 'reference','title'], inplace=True)\n",
    "df_data_file_null_out = df_data_file_null_out.applymap(lambda x: None if x == '[None]' or x == '[]' or x == 'None' else x)\n",
    "df_data_file_null_out.dropna(inplace=True)\n",
    "df_data_file_null_out.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply -log transformation to all columns\n",
    "df_transformed = df_data_file_cutoff.applymap(lambda x: -np.log(x) if np.issubdtype(type(x), np.number) and x > 0 else x)\n",
    "\n",
    "# Display the transformed DataFrame\n",
    "print(df_transformed.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the 1st and 99th percentiles\n",
    "lower_percentile = df_data_file_cutoff['auth-keywords'].value_counts().quantile(0.01)\n",
    "upper_percentile = df_data_file_cutoff['auth-keywords'].value_counts().quantile(0.99)\n",
    "\n",
    "# Filter the DataFrame to keep only the rows with 'auth-keywords' within the desired range\n",
    "df_data_file_cutoff = df_data_file_cutoff[\n",
    "  df_data_file_cutoff['auth-keywords'].map(df_data_file_cutoff['auth-keywords'].value_counts()).between(lower_percentile, upper_percentile)\n",
    "]\n",
    "\n",
    "# Display the shape of the new DataFrame\n",
    "df_data_file_cutoff.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data_file_subject = pd.read_csv('data_pandas/1_features_drop_null.csv', encoding='utf-8')\n",
    "df_data_file_subject.shape\n",
    "df_data_file_subject.describe(include='all')\n",
    "\n",
    "# Ensure the subjects column contains valid string representations of lists\n",
    "def safe_literal_eval(val):\n",
    "\ttry:\n",
    "\t\treturn ast.literal_eval(val)\n",
    "\texcept (ValueError, SyntaxError):\n",
    "\t\treturn []\n",
    "\n",
    "# Split the subjects column into multiple rows\n",
    "df_data_file_subject['subjects'] = df_data_file_subject['subjects'].apply(safe_literal_eval)\n",
    "\n",
    "df_data_file_subject = df_data_file_subject.explode('subjects').reset_index(drop=True)\n",
    "\n",
    "# Clean subject values\n",
    "df_data_file_subject['subject'] = (\n",
    "    df_data_file_subject['subjects']\n",
    "    .str.replace(r'[\\[\\]\\\"\\']', '', regex=True)  # Remove unwanted characters\n",
    "    .str.strip()                                # Trim whitespace\n",
    "    .str.replace(r'\\(.*?\\)', '', regex=True)    # Remove parentheses and their contents\n",
    "    .str.lower()                                # Convert to lowercase\n",
    ")\n",
    "\n",
    "def is_valid_utf8(text):\n",
    "    try:\n",
    "        text.encode('utf-8')\n",
    "        return True\n",
    "    except UnicodeEncodeError:\n",
    "        return False\n",
    "\n",
    "# Filter rows with only valid UTF-8 characters\n",
    "df_data_file_subject['subject'] = df_data_file_subject['subject'].apply(lambda x: x if is_valid_utf8(x) else np.nan)\n",
    "\n",
    "# Drop the original subjects column (if required)\n",
    "df_data_file_subject.drop(columns=['subjects'], inplace=True)\n",
    "\n",
    "df_data_file_subject.dropna(inplace=True)\n",
    "df_data_file_subject.drop_duplicate(inplace=True)\n",
    "\n",
    "import json\n",
    "with open('clustering_subject.json', encoding='utf-8') as file:\n",
    "  mapping_subjects = json.load(file)\n",
    "  \n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "from fuzzywuzzy import fuzz\n",
    "\n",
    "subject_cluster_mapping = {cluster_data['Cluster']: set(cluster_data['Subjects']) for cluster_data in mapping_subjects}\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def map_subjects(subject):\n",
    "    subject = subject.lower()\n",
    "    tokens = subject.split()\n",
    "    stemmed_tokens = [stemmer.stem(token) for token in tokens]\n",
    "\n",
    "    for cluster, subjects in subject_cluster_mapping.items():\n",
    "        for cluster_subject in subjects:\n",
    "            cluster_tokens = cluster_subject.lower().split()\n",
    "            stemmed_cluster_tokens = [stemmer.stem(token) for token in cluster_tokens]\n",
    "\n",
    "            # Exact match\n",
    "            if set(stemmed_tokens) == set(stemmed_cluster_tokens):\n",
    "                return cluster\n",
    "\n",
    "            # Fuzzy matching based on token similarity\n",
    "            fuzzy_ratio = fuzz.token_sort_ratio(subject, cluster_subject)\n",
    "            if fuzzy_ratio > 80:\n",
    "                return cluster\n",
    "\n",
    "    return 'Other'\n",
    "\n",
    "df_data_file_subject_clustering['subject-cluster'] = df_data_file_subject_clustering['subject'].apply(lambda x: map_subjects(x.strip()))\n",
    "\n",
    "df_data_file_subject_clustering.drop(columns=['subject'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "human_readable_mapping = {\n",
    "    'ASJC': 'all subject journal classification',\n",
    "    'SUBJABBR': 'subject abbreviation',\n",
    "    'EMCLASS': 'emerging class',\n",
    "    'FLXCLASS': 'flexible classification',\n",
    "    'CPXCLASS': 'complex classification',\n",
    "    'CABSCLASS': 'cabinet classification',\n",
    "    'GEOCLASS': 'geographic classification',\n",
    "    'ENCOMPASSCLASS': 'encompass classification'\n",
    "}\n",
    "\n",
    "# Replace abbreviations with human-readable terms\n",
    "df_data_file_classification['classifications'] = df_data_file_classification['classifications'].replace(human_readable_mapping)\n",
    "df_data_file_classification['classifications'] = df_data_file_classification['classifications'].apply(ast.literal_eval)\n",
    "df_data_file_classification = df_data_file_classification.explode('classifications').reset_index(drop=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
